\chapter{Программное обеспечение контроля сборки}

С целью контроля всей системы автоматизированной сборки было разработано специальное программное обеспечение (Qt~приложение) для ПК. Всё необходимое аппаратное обеспечение подключается по интерфейсу USB.

\section{Общая структура приложения}

Контроль подсистемами движения, наблюдения и отрицательного давления интегрированы в одно приложение, именуемого в дальнейшем PSAuto (от автоматизированной сборки PS модуля). Приложение написано на языке программирования C++ и использует Qt~фреймворк версии 4.8.7. Схематически интеграция трёх подсистем автоматизированной сборки изображена на Рисунке \ref{fig:general_app_structure}.

\begin{figure}[ht]\centering
\includegraphics[width=1\linewidth]{Data/Control_Software/Whole_system_diagram_(English).png}
\caption{Схематичческое изображение интеграции подсистемами движения, наблюдения и отрицательного давления с помощью Qt~приложение PSAuto.}
\label{fig:general_app_structure}
\end{figure}

\subsection{Шаблон проектирования Модель-Представление-Контроллер}

Структура приложения построено согласно шаблону проектирования Модель-Представление-Контроллер (Model-View-Controller (MVC)). В нём рассматриваются три основных типа объектов: объекты \emph{модели}, объекты \emph{представления} и объекты \emph{контроллера}. При разработке приложения важным шагом является выбор или создание пользовательских классов для объектов, которые попадают в одну из этих трех групп. Каждый из трех типов объектов отделен от других абстрактными границами. Шаблон определяет не только объекты ролей в приложении, но также определяет способ взаимодействия объектов друг с другом \cite{apple_MVC}. Ключевым моментом MVC является то, что объекты Представления и Контроллера зависят от объектов Модели, однако объекты Модели в свою очередь не зависит от них. Взаимодействие данных типов объектов схематически показано на Рисунке \ref{fig:mvc_general}.

\begin{figure}[ht]\centering
\includegraphics[width=0.7\linewidth]{Data/Control_Software/MVC_general.png}
\caption{Модель, Представление and Контроллер (MVC) относительно пользователя (классическая взаимодействие).}
\label{fig:mvc_general}
\end{figure}

На Рисунке \ref{fig:mvc_general} показано взаимодействие объектов классической архитектуры MVC. Существует различные варианты данной диаграммы с точки зрения взаимодействия между объектами MVC. Это зависит от типа приложения и его реализации. Например, в приложении PSAuto данная диаграмма будет иметь вид как на Рисунке \ref{fig:mvc_custom}.

\begin{figure}[ht]\centering
\includegraphics[width=0.7\linewidth]{Data/Control_Software/MVC_custom.png}
\caption{Модель, Представление and Контроллер (MVC) относительно пользователя (взаимодействие в приложении PSAuto).}
\label{fig:mvc_custom}
\end{figure}

По сравнению с классической структурой MVC как на рисунке \ref{fig: mvc_general}, Модель напрямую не информирует/обновляет Представление. Эта информация проходит через Контроллер, который действует как посредник между Моделью и Представлением. Контроллер часто отвечает за то, чтобы Представление имело доступ к объектам Модели, которые им нужно отображать, и выступает в качестве канала, через который объект Представления узнаёт об изменениях в Модели. Объекты Контроллера также могут выполнять задачи настройки приложения и управлять рабочими процессами других объектов \cite{apple_MVC}.

В зависимости от логики и требований приложения, Модель, Представление и Контроллер следуют определённым требованиям и обладают определёнными свойствами.

\emph{Модель} является центральным компонентом системы. Она напрямую работает с данными, определяя логику и правила работы приложения. Более того, как в случае с PSAuto, Модель ещё и регулирует взаимодействие между программным и аппаратным обеспечением. Например, в приложении PSAuto есть объект Модели \emph{ConradModel}, который отвечает за взаимодействие с релейной картой, контролирующей линии отрицательного давления системы. Данный объект полностью согласуется с шаблоном проектирования MVC как объект Модели, так как не зависит от других классов, однако другие классы зависят от него, используя его возможности для своего функционирования.

\emph{Представление} объединяет все объекты, отвечающие за графический интерфейс приложения: окна, вкладки, формы, поля, кнопки и т.п. Объекты представления отвечают за отображение информации, предоставляемой объектами модели, а также предоставляет пользователю вносить изменения в работы приложения (осуществлять контроль процесса сборки). Например, в приложении PSAuto одна из задач объекта \emph{AssemblyModuleAssembler} контролирует отображение информации о состоянии линий отрицательного давления в реальном времени, а также элементы их управления. Однако, он только отображает элементы контроля и передаёт действия пользователя объектам Контроллера, но сам не участвует в обработке этих действий.

Контроллер в основном ответственный за приём и обработку поступающих данных, генерацию команд для Модели и Представления. Например, в приложении PSAuto объект \emph{ConradManager} предоставляет весь необходимый функционал для контроля линиями отрицательного давления и получения информации об их текущем статусе. Также стоит отметить, что очень часто не существует прямой связи между Моделью и Представлением, как показано на Рисунке \ref{fig:mvc_general}. Вместо этого Контроллер выступает в качестве промежуточного звена, передающего информацию между Моделью и Представлением.

\subsection{OpenCV library}

Множество алгоритмов, обеспечивающий функционал приложения PSAuto, реализуется при помощи библиотеки с открытым кодом~---~OpenCV (\textit{Open Source Computer Vision}). Это кроссплатформенная библиотека алгоритмов, в основном ориентированных на компьютерное зрение и обработку изображений в реальном времени (OpenCV -- от англ. Computer Vision). Данная библиотека была разработана в исследовательском центре Intel в Нижнем Новгороде (Россия). Она распространяется на условиях лицензии BSD и может быть свободна использована в академических целях.

OpenCV была специально ориентирована на высокую вычислительную эффективность и, как уже упоминалось, ориентированность на работу в реальном времени. Она полностью реализована на языке программирования C++ и способна использовать преимущества многоядерных процессорных систем. Также она поддерживает возможность автоматической оптимизации под архитектуру продуктов Intel с помощью библиотек \textit{Integrated Performance Primitives (IPP)}, которые состоят из низкоуровневых оптимизированных подпрограмм в различных алгоритмических областях~\cite{kaehler2016learning}.

Одна из основных задач библиотеки OpenCV -- предоставить простую и понятную в использовании инфраструктуру для работы с приложениями компьютерного зрения, что оказывает серьёзную поддержку в разработке достаточно сложных приложений данной области. Библиотека насчитывает более 500 функций, охватывающих множество областей компьютерного зрения и обработки изображений. Ниже приведены основные функции библиотеки OpenCV:

\begin{itemize}
\setlength\itemsep{-0.5em}
\item Ввод/вывод изображений и видео (открытие изображений из файла или непосредственное получение от камеры, вывод изображений/видеофайлов)
\item Операции линейной алгебры над векторами и матрицами (умножение, определители, собственные значения, сингулярное разложение).
\item Различные динамические структуры данных (списки, очереди, наборы, деревья, графики).
\item Основы обработки изображений (фильтрация, обнаружение краев, определение угла, выборка и интерполяция, преобразование цвета, морфологические операции, гистограммы и т.д.).
\item Структурный анализ (связанные компоненты, обработка контуров, дистанционное преобразование, совпадение шаблонов, преобразование Хафа, многоугольное приближение, линейная подгонка, эллиптическая подгонка, триангуляция Делоне).
\item Калибровка камеры (поиск и отслеживание шаблонов калибровки, калибровка, оценка фундаментальной матрицы, оценка гомографии, стереозапись).
\item Анализ движения (оптический поток, сегментация движения, отслеживание).
\item Распознавание объектов (метод собственных значений, метод скрытой Марковской Модели).
\item Основные элементы графического интерфейса (отображение изображения/видео, управление клавиатурой и мышью, полосы прокрутки).
\item Маркировка изображений (линии и другие фигуры, текстовые подписи).
\end{itemize}

Применение функционала библиотеки будет описано в следующих параграфах.

\section{Распознавание образов}

Распознавание образов является важной функцией всей автоматизированной системы сборки, в частности - подсистемы наблюдения. Она обеспечивает программное обеспечение информацией расположении платформы и компонентов модуля в пространстве и их планарной ориентации. С помощью этой информации программное обеспечение способно вычислить, куда перемещать каждый компонент модуля во время сборки. По определению, планарная ориентация -- это вращательная ориентация объекта в горизонтальной плоскости (XY в данном случае) \cite{AutomatedAssembly_tutorial}.

Для системы автоматизированной сборки планарная ориентация проводится в два этапа: независимое определение \emph{положения} и \emph{ориентации} каждого из четырёх маркеров на углах сенсора и совокупность положений четырёх маркеров, из которой вычисляется конечная точная ориентация всего сенсора. Это делается путем обработки изображений \emph{точных маркеров} в углах сенсоров (Рисунок \ref{fig:fiducial_marker}), полученных подсистемой наблюдения. Маркеры точно расположены относительно полосок/пикселей сенсора, следовательно, узнав точное положение маркеров, точное позиционирование полосок/пикселей гарантировано. В качестве входных данных алгоритм берет необработанные изображения и возвращает значения положения и ориентации маркеров, расположенных в углах сенсорах PS модуля. Алгоритм распознавания образов использует пакет библиотеки \emph{OpenCV}. Ниже перечислены этапы распознавание положения и ориентации сенсора:

\begin{enumerate}
\setlength\itemsep{-0.5em}
\item Предварительная обработка изображения, полученного с камеры.
\item Определение позиции и планарной ориентации маркера.
\item Определение приблизительного положения следующего маркера и/или конечное вычисление планарной ориентации сенсора.
\end{enumerate}

\begin{figure}[ht]\centering
\includegraphics[width=0.7\linewidth]{Data/Control_Software/Fiducial_marker.png}
\caption{Принятые за основу сравнения маркеры на углах сенсора PS модуля.}
\label{fig:fiducial_marker}
\end{figure}

\subsection{Предварительная обработка изображения, полученного с камеры}

Необработанное изображение с камеры сперва преобразуется из цветного в изображение в оттенках серого, что известно как "\emph{grayscaling}" (от англ. grayscale -- оттенки серого) в библиотеке OpenCV. Пиксели изображения в оттенках серого содержат информацию об интенсивности в виде одного числа от 0 до 255, характеризующего степень темноты оттенка серого, в отличие от информации об интенсивности и цвете, содержащейся в пикселях цветного изображения. Поскольку маркеры основаны не на цветах, а на простых формах (окружность, угол), то информация о цвете не является полезной и потому не учитывается. Вместо этого изображение в оттенках серого преобразуется в \emph{бинарное} изображение, у которого каждый пиксель либо белый, либо чёрный. Данная процедура получила название \emph{"Thresholding"}, от англ. threshold -- порог (пороговое значение). Операция Thresholding заключается в простом конвертировании каждого пикселя изображения в оттенках серого в белый (чёрный) пиксель, если его интенсивность выше(ниже)  заданного порогового значения. Данная операция призвана уменьшить различие между изображениями маркеров идентичных сенсоров из-за случайного \emph{шума}, вызванного пылью и случайными различиями на поверхностях маркеров сенсора. Примеры изображения маркера в оттенках серого и его бинарных изображений для различных пороговых значений показаны на Рисунке \ref{fig:threshold}. Оптимальное значение порогового значения зависит от общего уровня освещённости вокруг места проведения сборки и контраста маркера на поверхности сенсора. На данный момент, типичное значение порогового значения составляет около 90, что приблизительно соответствует освещению лаборатории лампами дневного света.

\begin{figure}[ht]\centering
\includegraphics[width=0.9\linewidth]{Data/Control_Software/Threshold.png}
\caption{Применение операции Thresolding на изображении маркера}
\label{fig:threshold}
\end{figure}

\subsection{Определение позиции и планарной ориентации маркера}

Положение и ориентация маркера на бинарном изображении определяется с использованием стандартным для такой ситуации методом обработки изображений, известным как \emph{сопоставление шаблонов}. Библиотека OpenCV предоставляет одноимённую функцию для выполнения таких задач. Метод находит ту часть исследуемого изображении, которая больше всего походит на шаблонное изображение. В случае автоматизированной сборки исследуемым изображением является преобразованное в бинарное изображение полученное с камеры, а шаблонным -- заранее подготовленное бинарное изображение маркера. 

Template matching proceeds by iteratively superimposing the template image at each point of the master image and calculating a metric which describes the similarity of the template image and the portion of the master image with which it coincides. The OpenCV package provides multiple options for the metric. Similar results are observed for each possible metric with the chosen metric based on the normalised squared difference between the intensities of coincident pixels of the master image and superimposed template.

Сопоставление шаблонов начинает с итеративного сопоставления шаблонного изображения в каждой точке исследуемого изображения и высчитывает определённое числовое значение, или метрику, показывающее степень схожести шаблонного изображение с частью исследуемого изображения, которая соответствует данной точки. В приложении PSAuto эта точка является левой-верхней точкой данной части исследуемого изображения. Библиотека OpenCV предоставляет множество вариантов вычисления значения этой метрики. Аналогичные результаты наблюдаются для каждой из возможных метрик, поэтому была выбрана наиболее скоростная по вычислительной сложности -- метрика, основанная на нормированной квадратичной разности интенсивностей налагающихся пикселей исследуемого изображения и шаблона:

\begin{center}
$R(x,y)=\dfrac{\sum_{x',y'}^{}(T(x',y')-I(x+x',y+y'))^{2}}{\sum_{x',y'}^{}\sqrt{\sum_{x',y'}^{}T(x',y')^{2}\cdot\sum_{x',y'}^{}I(x+x',y+y')^{2}}}$
\end{center}
где I означает \emph{исследуемое изображение}, T -- \emph{шаблонное изображение} and R -- \emph{результирующая метрика}.


Данная метрику можно найти под именем CV\_TM\_SQDIFF\_NORMED в библиотеке OpenCV. Точка исследуемого изображения, в которой метрика достигнет минимального значения показывает наиболее вероятное положение шаблонного изображения, маркера в случае автоматизированной сборки. На Рисунке \ref{fig:template_matching} изображен процесс определение положения маркера с помощью сопоставления шаблона и его результат с использованием маркера на пробном сенсоре. После нахождения наиболее вероятного положение маркера алгоритм выделяет его белым прямоугольником на исследуемом изображении. Наблюдаемое на Рисунке \ref{fig:template_matching} местоположение близко соответствует ожиданиям.

\begin{figure}[ht]\centering
\includegraphics[width=0.9\linewidth]{Data/Control_Software/Template_matching.png}
\caption{Методика сопоставления шаблонов проиллюстрирована на левом изображении. Красные стрелки указывают направления итеративного расчет метрики в каждой точке основного изображения. Результат работы алгоритма сопоставления шаблонов при использовании тестовых изображений показан на правом изображении. Наиболее вероятное расположение маркера в главном изображении обозначается красным прямоугольником.
}
\label{fig:template_matching}
\end{figure}

In order to deduce the orientation of the marker in the plane transverse to the optical axis of the camera, the matching procedure is repeated iteratively with different rotational transformations applied to the master image. For each iteration, the minimal value of the metric is recorded with the minimal metric value across all iterations denoted as $\alpha$. The planar orientation of the sensor is estimated as $-\alpha$. In Figure \ref{fig:template_rotation} a schematic illustrating the determination of the orientation is shown. A graph of the resultant minimised metric values versus the size of the angular transformation applied to the master image is shown on the right image of the Figure \ref{fig:template_rotation}. The graph corresponds to a test extraction performed with images of a dummy sensor where the sensor in the master image had a planar orientation of $\approx$ 3.5 degrees. A clear minimum is observed at $\approx$ 3.5 demonstrating the method's validity. More precise determination of the sensor orientation can be achieved when factors such as ambient light conditions, image focus and marker design are further optimised. The best accuracy reached during tests was $\approx$ 0.025 degrees.

Чтобы вычислить ориентацию маркера в плоскости, перпендикулярной оптической оси камеры, процедура сопоставления шаблонов итеративно повторяется с различными вращательными преобразованиями для разных углов (в установленных пределах с определённым шагом), применяемыми к части исследуемого изображения, где предварительно маркер уже был найден без информации о его ориентации на плоскости. В каждой итерации сохраняется минимальное значение метрики. По окончанию прохождения по всем углам в установленных пределах, строится график 

\begin{figure}[ht]\centering
\includegraphics[width=0.9\linewidth]{Data/Control_Software/Template_rotation.png}
\caption{A schematic illustrating the estimation of the sensors planar orientation is shown on the left image. The red arrows indicate the iterative rotational transformations applied to the master image. graph of the minimised metric value versus the angular transformation applied to the master image is shown on the right image. A clear minimum at $\approx$ 3.5 degrees is observed.}
\label{fig:template_rotation}
\end{figure}

\subsection{Определение приблизительного положения следующего маркера и/или конечное вычисление планарной ориентации сенсора}
The procedure described in step 2 is repeated at each sensor corner. The planar orientations determined at given corner are used to set the direction of movement needed for the motion stage to automatically travel to an adjacent corner. The final position and orientation of the sensor is determined by a $\chi^{2}$ fit to the four (x,z) points. The orientation determined from the fit is cross-checked with the estimations of the orientation at the corners which accuracy could be not far from $\chi^{2}$ deduced. If there is agreement between the fit and four corner orientations, the fit results are used.

Detailed Flow Chart of the pattern recognition is shown in the Appendix \ref{fig:uml_uml_pattern_recognition}.

\section{Application functionality}

The main window of the application has several buttons and check boxes on top of the window and a number of tabs including: Finder, Threshold, Assembly, Autofocus, Motion Manager and others.

\begin{enumerate}

\item \emph{Finder.} A simple tab the only purpose of each is to show the last acquired image of the camera. This tab also provide a possibility to save current image.

\item \emph{Threshold.} The result of the Threshold operation highly depends on the ambient light thus each new test required threshold value calibration before starting it. This tab is specifically created to control the applying threshold value. It provides the possibility to configure this value and gives an immediate respond by applying Threshold operation on the last acquired image and showing the result under the grayscale original image. The screenshot of the Threshold tab is shown on the Figure \ref{fig:threshold_screenshot}.

\begin{figure}[ht]\centering
\includegraphics[width=0.7\linewidth]{Data/Control_Software/Threshold_screenshot.png}
\caption{Screenshot of the Threshold tab of the PSAuto application.}
\label{fig:threshold_screenshot}
\end{figure}

\item \emph{Assembly.} The primary tab of the application (Figure \ref{fig:assembly_screenshot}). It contains four image boxes on the left, control tools on the right and motion stage real time status information on the bottom. Left-top image box contains last acquired image. Left-bottom image box -- Thresholded (binary) image of it. Right-bottom image box shows the template image. Finally, right-top image box represents the graph of the last pattern recognition metric distribution of the template matching metric along theta value.

\begin{figure}[ht]\centering
\includegraphics[width=0.7\linewidth]{Data/Control_Software/Assembly_screenshot.png}
\caption{Screenshot of the Assembly tab of the PSAuto application.}
\label{fig:assembly_screenshot}
\end{figure}

On the right side of the tab there are a number of control tools (beginning from the top one):

\begin{itemize}
\setlength\itemsep{-0.5em}
\item Two buttons to move the pickup tool to the absolute or relative position which is written in the forms beside buttons respectively.
\item A set of radio-buttons choosing type of markers on samples and the pattern recognition mode.
\item A set of buttons to control the relay card of vacuum lines. Beside each button there is a real time indicator of the current vacuum line status.
\item The last tool in this tab provides the possibility to conduct tests of the system. It consists of several forms for input test information and a button to start the test.
\end{itemize}

\item \emph{Auto focus.} Another nice tool of the application is autofocus. It place an essential role in the pattern recognition as soon as its accuracy directly depends on the quality of acquired image. There are two features provided by this tab: find the focus position of the camera and move to this position. To find the focus position of the camera it is required to set the step size along Z-axis and the number of steps. As smaller the step is, the more precise focus position will be identified. The idea of auto focus detection is similar to best theta detection in the pattern recognition. For each step position it saves the metric data representing the blur of the acquired image thus in the end the software can find the position with least blur on the image comparing saved metrics for each image.

This algorithm can also be used for another purpose. As soon as the camera is constantly fixed on the robotic arm, it is possible to make relative Z-axis distance measurement. For instance, that can be used for measuring the glue layer thickness of an assembled modules.

\item \emph{Motion Manager.}
This tab provides the control of the motion and rotation stages. Among its functionality are:

\begin{itemize}
\setlength\itemsep{-0.5em}
\item Independent control of X,Y,Z axis and rotation stage.
\item Self-calibration.
\item Real time monitoring of the status of stepper motors of each axis and rotation stage.
\end{itemize}
\end{enumerate}


